#!/bin/bash
set -euo pipefail

# Creates individual GitHub issues for each failing test
# Usage: ./create-test-issues.sh <owner> <repo> <run-id> <failures-json>

OWNER="${1:-}"
REPO="${2:-}"
RUN_ID="${3:-}"
FAILURES_JSON="${4:-test-failures.json}"

if [[ -z "$OWNER" || -z "$REPO" || -z "$RUN_ID" ]]; then
    echo "Usage: $0 <owner> <repo> <run-id> <failures-json>"
    echo "Example: $0 deploymenttheory terraform-provider-microsoft365 123456 test-failures.json"
    exit 1
fi

if [[ ! -f "$FAILURES_JSON" ]]; then
    echo "Error: Failures JSON file not found: $FAILURES_JSON"
    exit 1
fi

DATE=$(date -u +"%Y-%m-%d")
WORKFLOW_URL="https://github.com/${OWNER}/${REPO}/actions/runs/${RUN_ID}"

# Check if JSON is empty or has no failures
FAILURE_COUNT=$(jq 'length' "$FAILURES_JSON")

if [[ "$FAILURE_COUNT" -eq 0 ]]; then
    echo "✅ No test failures found"
    exit 0
fi

echo "Found ${FAILURE_COUNT} failing test(s)"

# Process each test failure
for i in $(seq 0 $((FAILURE_COUNT - 1))); do
    TEST_NAME=$(jq -r ".[$i].test_name" "$FAILURES_JSON")
    CATEGORY=$(jq -r ".[$i].category" "$FAILURES_JSON")
    SERVICE=$(jq -r ".[$i].service" "$FAILURES_JSON")
    CONTEXT=$(jq -r ".[$i].context" "$FAILURES_JSON")
    
    # Build service path for better context
    SERVICE_PATH="${CATEGORY}"
    if [[ -n "$SERVICE" && "$SERVICE" != "null" ]]; then
        SERVICE_PATH="${CATEGORY}/${SERVICE}"
    fi
    
    echo ""
    echo "Processing failure: ${TEST_NAME}"
    
    # Create issue title - standardized format for de-duplication
    ISSUE_TITLE="Test Failure: ${TEST_NAME}"
    
    # Check if issue already exists
    echo "Checking for existing test failure report..."
    EXISTING_ISSUE=$(gh issue list \
        --repo "${OWNER}/${REPO}" \
        --state open \
        --label "test-failure" \
        --search "in:title \"Test Failure: ${TEST_NAME}\"" \
        --json number,title \
        --jq ".[0].number" || echo "")
    
    if [[ -n "$EXISTING_ISSUE" && "$EXISTING_ISSUE" != "null" ]]; then
        echo "Test failure report already exists: #${EXISTING_ISSUE}"
        echo "Updating with latest occurrence..."
        
        # Add comment to existing issue with latest occurrence
        COMMENT_BODY="## Test Failure Update: Recurring

**Date:** ${DATE}  
**Workflow Run:** [${RUN_ID}](${WORKFLOW_URL})  
**Service Path:** \`${SERVICE_PATH}\`

### Latest Failure Context

\`\`\`
${CONTEXT}
\`\`\`

### Assessment

This test has failed again in the latest nightly run. Possible causes:
- Unresolved issue from previous occurrence
- New regression introduced
- Flaky test (environment instability)
- Test requires maintenance/update

**Next Steps:** 
1. Review failure pattern to determine if this is a persistent issue
2. If persistent (3+ consecutive failures), consider escalating to incident
3. If intermittent, investigate flaky test root cause

---
*Automated update by test monitoring system*"

        gh issue comment "$EXISTING_ISSUE" \
            --repo "${OWNER}/${REPO}" \
            --body "${COMMENT_BODY}"
        
        # Update labels to indicate recurring issue
        gh issue edit "$EXISTING_ISSUE" \
            --repo "${OWNER}/${REPO}" \
            --add-label "recurring"
        
        echo "Updated test failure report #${EXISTING_ISSUE}"
        continue
    fi
    
    # Create new test failure report
    echo "Creating new test failure report..."
    
    ISSUE_BODY="<!-- Automated test failure report generated by nightly test pipeline -->

## Test Failure Report

**Status:** Under Investigation  
**Test Name:** \`${TEST_NAME}\`  
**Service Area:** \`${SERVICE_PATH}\`  
**First Detected:** ${DATE}  
**Workflow Run:** [${RUN_ID}](${WORKFLOW_URL})

## Failure Details

### Test Information

- **Test Path:** \`${TEST_NAME}\`
- **Test Type:** Acceptance Test
- **Service:** ${SERVICE_PATH}
- **Failure Pattern:** First Occurrence

### Failure Context

\`\`\`
${CONTEXT}
\`\`\`

## Impact Assessment

**Pre-Production Environment:**
- ⚠️ Test suite quality gate failing
- ⚠️ Potential regression if deployed to production
- ✅ No customer impact (pre-production only)

**Severity Classification:**
- **Not Yet an Incident** - No production service disruption
- **Could Escalate** - If persists or blocks critical deployments

## Investigation Required

### Initial Analysis Checklist

- [ ] Review [workflow logs](${WORKFLOW_URL}) for details
- [ ] Identify root cause (code change, environment, flaky test)
- [ ] Determine severity and regression risk
- [ ] Check if this affects production release readiness

### Classification Decision

After investigation, classify this failure:

- [ ] **Flaky Test** - Intermittent failure, test needs improvement
- [ ] **Environment Issue** - Test infrastructure problem
- [ ] **Genuine Bug** - Code defect requiring fix
- [ ] **Test Maintenance** - Test needs updating for new behavior

### Escalation Criteria

**Escalate to INCIDENT if:**
- Test fails 3+ consecutive times (persistent failure)
- Failure blocks critical production deployment
- Failure indicates production service degradation
- Security or data integrity concern

## Resolution Actions

- [ ] Fix identified root cause
- [ ] Verify fix locally and in CI/CD
- [ ] Document findings
- [ ] Close report when test passes
- [ ] Update test if maintenance needed

## Resources

- [View Workflow Run](${WORKFLOW_URL})
- [View Test Coverage](https://codecov.io/gh/${OWNER}/${REPO})
- [Test Source Code](../../)

---

**Management Notes:**

This is a test failure report, not an incident. It will be automatically updated if the test continues to fail.

**Closure Criteria:**
- Test passes in subsequent nightly run
- Root cause identified and documented
- OR classified as \`wontfix\` with justification

*Automated report by test monitoring system*"

    ISSUE_URL=$(gh issue create \
        --repo "${OWNER}/${REPO}" \
        --title "${ISSUE_TITLE}" \
        --body "${ISSUE_BODY}" \
        --label "test-failure,automated,needs-triage")
    
    echo "Created test failure report: ${ISSUE_URL}"
done

echo ""
echo "Test failure processing complete: ${FAILURE_COUNT} test failure(s) reported"

